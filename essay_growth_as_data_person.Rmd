---
title: "Essays"
author:
  - name: Sajal Shrestha 
    url: https://sajalshres.github.io
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Growth as a Data Person

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/pexels-growth-as-person.jpg")
```

Over the past few years, I have been fascinated by data science and wanted to advance my career as a Data Scientist. Before taking this class, I had limited experience with data science, analytics, and data management in general. Most of my experience comes from being a system engineer(DevOps). I had some programming experience in Python and Shell scripting that I use to automate manual tasks and deploy infrastructures. I was very doubtful about learning R programming as it is generally perceived as a challenging language with a steeper learning curve. I also heard about R being mentioned in multiple projects but never got the chance to use it personally. I am quite exhilarated to be part of the STA 518 course, as it has allowed me to expand on my programming and statistical skills. As I've advanced through the class activities, preparations and projects, I've learned quite a bit about R programming, from knowing the basic syntax to building large-scale data applications. The primary learning curve for grasping the language came from following through with class activities, preparing self-reflection in the portfolio, and building projects. If I had to pick class activities that significantly enhanced my skills, it would be `data-pipelines`, `ggplot`, `functions-intro`, and `shiny-apps`, among others. These activities helped me learn in-depth about creating R programs and using necessary packages like `tidyverse`, `testing`, `shiny`, etc.

The first significant growth I felt in my growth as a data person is learning how to manage data and process them. I feel that data ingestion from multiple sources like text, databases, JSON, and CSV are critical when designing a data-driven product and must-have skills when working on large-scale projects in enterprises. In my last job as a Software Engineer, I was responsible for building back-ends for an enterprise app. We had a dedicated data operation team responsible for maintaining ETL pipelines. The complex data engineering process very much intimated me, and it was one of the primary reasons to pursue my Master's. As I have advanced in this course, I have learned how to import data from sources like excel, CSV, text, yaml, and JSON. I have mastered identifying outlier, noise, and missing values and fixing them by either eliminating them or using other processing techniques like aggregation. I can also combine data and reorganize them to derive new information to highlight key features. Furthermore, I have also built an ETL command line script to process data in my final project, which helped simulate building complex data pipelines. The data-management section of my portfolio and ETL script demonstrates my ability to perform data cleansing and management.

My subsequent significant growth as a data-driven person is developing data-driven applications using libraries like shiny, shinydashboards, and plumber. As I begin my journey to build a shiny web application, I have little idea of how to build a large-scale web application. I scoured the internet to find the ideal content organization, but I could only find simple examples where all the source code was compiled into a fewer files. However, as I progressed, I learned how to break my code into multiple files and organize them into multiple folders. The content structure can be viewed in my [final project](https://github.com/sajalshres/sta-518-project). Also, I implemented a dashboard GUI layout that allows me to show visualizations such as bar charts, pie charts, and histograms and render geographical maps. I discovered reactive programming in Shiny, which allowed me to create interactive and dynamic UI based on observing certain events, such as UI input changes. For instance, in my final Airbnb project, I implemented an event-driven app where the data and visualizations are loaded dynamically when the user selects the city from the dropdown menu. Likewise, I implemented map widgets, where users can filter the data via the UI, and the maps react accordingly. I am also surprised by how intuitive and easy it was to create a web application in R comparatively. For example, I was able to build my portfolio within a few hours using days and learn to build a shiny app within a couple of days(even though I would spend the following few weeks troubleshooting and learning more about reactivity). If I were to build a similar application in Python or Java, it would take me weeks to implement them.

Overall, I am very content with the progress I have made in STA 518 course as a versatile data person and a decent R programmer. Throughout the course, I have challenged myself to explore different ideas on the project, collaboratively troubleshoot with the team, work on multiple hands-on tasks and develop my data analytical skills. I also value how the course was uniquely structured, which encouraged me to explore further and provided the autonomy to experiment instead of doing what was required to study. I know I've just scratched the surface in my expedition to become a data person. Still, I am confident about taking on any challenges in the future and continue exploring and learn things about R.
