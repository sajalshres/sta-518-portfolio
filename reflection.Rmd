---
title: "Final Grade Reflection"
author:
  - name: Sajal Shrestha 
    url: https://sajalshres.github.io
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 3
    toc_float: true
    highlight: breezedark
    highlight_downlit: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

```{r load-package-and-modules, message = FALSE}
library(here)
library(tidyverse)
library(rmarkdown)
library(GGally)
library(waffle)
```

# 1. What Grade Did You Earn?

I have completed all the class activities, readings, and preparations on the blackboard. As I have advanced through the course, I've learned from basic to advanced concepts of R programming. I have applied all my learning in the portfolio and final project, showcasing all the required goals. Likewise, my final project follows all the major guidelines and creative constraints collaboratively with colleagues. I think R has a very steep learning curve, but I believe I've made good progress, and I am optimistic about being a decent R programmer at the end of this course. Even though I know that I've just scratched the surface of the world of R, I am confident that I can take on any challenges/projects in the future, and I look forward to exploring further. Therefore, I would grade myself anÂ **'A.'**

# 2. How Did You Demonstrate That Grade?

## 2.1 Import, manage and clean data

I can import data from various sources such as text, csv, excel files and load it into the R data structure such as modern `tibble` data frame or base `data.frame`. I can manage the dataset using the functions such as `mutate()`, `select()`, `filter()`, `summarise()` and `arrange()` in the **dplyr** package.

For my final project, I worked on the Airbnb dataset for multiple cities to process the property listings, neighbourhoods, and reviews. To achieve this, I created a custom scrape function: [scrape.R](https://github.com/sajalshres/sta-518-project/blob/main/modules/scrape.R) and an [etl(extract-transform-load)](https://github.com/sajalshres/sta-518-project/blob/main/etl/main.R) command line tool for managing data. For portfolio demonstration, I am using the **Chicago** dataset as it is very close to Grand Rapids, and I plan on visiting someday.

### 2.1.1 Import Data

I imported the Airbnb data set for the Chicago city in to the dataframe for my final project. Below is the code to import into the dataframe.

```{r import-data-airbnb, message=FALSE, code_folding=FALSE}
download_files <- function(base_url, file_names) {
  for (file_name in file_names) {
    dest_file <- paste("data", file_name, sep = "/")

    print(paste("Downloading file:", dest_file))
    if (!file.exists(dest_file)) {
      httr::GET(
        url = paste(base_url, file_name, sep = "/"),
        httr::write_disk(paste("data", file_name, sep = "/"), overwrite = TRUE),
        httr::progress()
      )
    } else {
      print(paste("Download skipped. File aready exists:", dest_file))
    }
  }
}

# Set the base url
airbnb_base_url <-
  "http://data.insideairbnb.com/united-states/il/chicago/2022-06-10/data"

# Download the files
download_files(
  base_url = base_airbnb_url,
  file_names = c("listings.csv.gz", "reviews.csv.gz")
)

# Import data
listings_raw <-
  readr::read_csv(here::here("data", "listings.csv.gz"))

reviews_raw <- readr::read_csv(here::here("data", "reviews.csv.gz"))
```

I imported the OGR vector maps for generating spatial maps of neighbourhoods.

```{r import-data-airbnb-geojson, message=TRUE, code_folding=FALSE}
neighbourhoods_ogr <- rgdal::readOGR("data/chicago_neighbourhoods.geojson")
```

I can also import excel files using `readxl:read_excel` function

```{r import-data-excel-files, message=TRUE}
amazon_novels <- readxl::read_excel(here::here("data", "AmazonBooks.xlsx"))
```

### 2.1.2 Manage Data

The Airbnb dataset has more than 80 columns. I choose only the required columns and removed unwanted columns using `dplyr::select` function.

```{r manage-data-airbnb}
# Manage listings data
listings <- listings_raw %>%
  # Isolate required columns
  select(
    id,
    name,
    description,
    neighbourhood_cleansed,
    latitude,
    longitude,
    property_type,
    room_type,
    accommodates,
    bathrooms_text,
    bedrooms,
    beds,
    amenities,
    price
  )
  
# Manage hosts data
hosts <- listings_raw %>%
  # Select all columns that starts with host
  select(id, dplyr::starts_with("host")) %>%
  # Rename id to listing_id
  rename(listing_id = id)
```

### 2.1.3 Clean Data

The Airbnb dataset has some columns that were in unexpected format. I have fixed those columns using `dplyr::mutate` function. Also, some of the columns had un-necessary pre-fixes. I renamed them using `dplyr::rename_with` and `dplyr::rename` function. Lastly, I relocated some columns using `dplyr::relocate` for better readability.

```{r clean-data-airbnb}
listings <- listings %>%
  # Convert bathrooms to number type
  mutate(
    bathrooms = parse_number(str_extract(bathrooms_text, "\\d*\\.?\\d+")),
    .after = beds
  ) %>%
  # Convert price to number type
  mutate(price = parse_number(price), ) %>%
  # Remove unwanted columns
  select(!bathrooms_text) %>%
  # Rename neighbourhood
  rename(neighbourhood = neighbourhood_cleansed)


hosts <- hosts %>%
  # Remove dublicate instances
  distinct(host_id, .keep_all = TRUE) %>%
  # Modify since format to date
  mutate(
    host_since = format(as.Date(host_since), "%m/%d/%Y"),
    .after = host_name
  ) %>%
  # Remove unwanted columns
  select(
    !c(
      "host_url",
      "host_thumbnail_url",
      "host_picture_url",
      "host_listings_count",
      "host_total_listings_count"
    )
  ) %>%
  # Relocate listing_id to second column
  relocate(listing_id, .after = host_id)
```

Some of the rows had empty strings or labelled with "N/A" value. I replaced them with `NA` and dropped the data for some of the columns which had `NA`.

```{r clean-data-airbnb-na-values}
# Replace empty values with NA
listings[listings == ""] <- NA
hosts[hosts == ""] <- NA

# Replace N/A values with NA
listings[listings == "N/A"] <- NA
hosts[hosts == "N/A"] <- NA

# Drop rows with NA for critical columns
listings <- listings %>% drop_na(c("id", "name"))
hosts <- hosts %>% drop_na(c("host_id"))
```

Also, I detected some noise in the dataset where the location was from wrong countries, city or state. I used `dplyr::filter` function to remove such noises.

```{r clean-data-airbnb-noises}
hosts <- hosts %>%
  filter(
    host_location == "Chicago, Illinois, United States"
  )
```

For my final project, since I was using custom [`etl`](https://github.com/sajalshres/sta-518-project/blob/main/etl/main.R) command-line utility with scraper built using `rvest`, I modified above code to support for different location based on city:

```{r clean-data-airbnb-noises-2}
hosts <- hosts %>%
    filter(
      host_location == names(which.max(table(host_location)))
    )
```

Furthermore, I combined listings, and hosts data sources as required using `dplyr::left_join()` function.

```{r manage-data-airbnb-combine}
combined_listings <- listings %>%
  select(id, price) %>%
  left_join(
    hosts %>% select(host_id, listing_id, host_name, host_location), 
    by = c("id" = "listing_id"), 
    suffix = c("", "")
  ) %>%
  drop_na(c("host_id"))

combined_listings %>% 
  paged_table(options = list(rows.print = 15))
```

### 2.1.4 Mini Project - World Happiness Report

For additional analysis, I also imported excel files from the [worldhappiness.report](https://worldhappiness.report/).

```{r manage-data-worldhappiness, message = FALSE}
# Import data
worldhappiness_2021 <- readr::read_csv(
  here::here("data", "world_happiness_report_2021.csv")
)
worldhappiness_all <- readr::read_csv(
  here::here("data", "world_happiness_report.csv")
)

worldhappiness_2021 <- worldhappiness_2021 %>%
  rename_with(.fn = ~ tolower(.x)) %>%
  select(!starts_with("Explained by")) %>%
  select(!"dystopia + residual") %>%
  rename_with(.fn = ~ gsub(" ", "_", .x))

worldhappiness_all <- worldhappiness_all %>%
  rename_with(.fn = ~ tolower(gsub(" ", "_", .x)))
```

```{r clean-up, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
clean_up_variables = c(
  "combined_listings",
  "hosts",
  "listings",
  "listings_raw",
  "reviews_raw",
  "worldhappiness_2021",
  "worldhappiness_all",
  "neighbourhoods_ogr"
)

rm(list = clean_up_variables)
```

### 2.1.5 Conclusion

I have met the objective of import, manage and clean data as illustrated in above sections. I have imported data from various sources such as csv, excel, geojson files to work on my objectives. I can isolate a subset of data from a large data source using base `data.frame` or `dplyr::select` as well as combine multiple data sources using `merge`, and joins in `dplyr` library. I can also restructure my data using `dplyr` functions such as `filter`, `group_by`, `rename`, `rename_with`, and `pipe`. I can create new information using `mutate` function based on existing columns and modify existing information such as converting formats, adding values etc for specific tasks.

------------------------------------------------------------------------

## 2.2 Create graphical displays and numberical summaries of data for exploratory analysis and presentations

For creating graphical visualizations, I have used `ggplot2` library. Using the library, I have generated numerous visualizations such as bar chart(`geom_bar`), box plots(`geom_boxplot`), histogram(`geom_histogram`), scatter plots(`geom_point`), dumbbell plots(`geom_segment`). In addition, I have also used `GGally` library's function `ggcorr()` to generate correlation matrix. Likewise, I utilized `waffle` library to generate waffle square chart using `geom_waffle()` function.

First of all, let's load the dataset into the `Global Environment` using `load_data.R` module.

```{r visual-load-modules, message = FALSE}
# Load all the data modules into the environment
source(here::here("modules", "load_data.R"))

# Load airbnb data
load_airbnb_data()
```

For my final project, I was interested in finding the missing values when importing the data as they are important when doing exploratory data analysis. At first, I used `dplyr::summarise_all()` method to find the count of missing values and transposed the missing values using built-in `t` transpose function.

```{r visual-missing-values-count}
missing_values_summary <- listings_raw %>%
  summarise(across(everything(), ~ sum(is.na(.))))

missing_values_summary <- as_tibble(
  cbind(
    columns = names(missing_values_summary), t(missing_values_summary))
  ) %>%
  rename(missing_values = V2) %>%
  mutate(across(missing_values, as.integer)) %>%
  arrange(desc(missing_values))

missing_values_summary %>% 
  paged_table(options = list(rows.print = 15))
```

Numerical summaries can be great to understand the dataset but graphical visualization can add more information. After generating summaries for the missing values, I created the visualization that demonstrate the proportion of missing values using `ggplot2` library.

```{r visual-missing-values, echo = TRUE, layout="l-body-outset", fig.height = 5}
missing_stat <- listings_raw %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num.isna = n()) %>%
  mutate(percentage = num.isna / total * 100)

levels <-
  (missing_stat %>% filter(isna == T) %>% arrange(desc(percentage)))$key

missing_stat %>%
  ggplot(aes(x = reorder(key, desc(percentage)), y = percentage, fill = isna)) +
  geom_bar(stat = "identity", alpha = 0.8) +
  scale_x_discrete(limits = levels) +
  scale_fill_manual(
    name = "",
    values = c("#0086ff", "#ff9d00"),
    labels = c("Present", "Missing")
  ) +
  # Apply custom theme
  theme(
    axis.text.x = element_text(size = 6, face = "bold"),
    axis.text.y = element_text(size = 6, face = "bold"),
    axis.title.x = element_text(size = 8),
    axis.title.y = element_text(size = 8),
    plot.title = element_text(size = 10, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 0.5),
  ) +
  labs(
    # Add x axis label
    x = "Columns",
    # Add y axis label
    y = "% of missing values",
    # Add title
    title = "Proportion of missing values in Airbnb data",
    # Add subtitle
    caption = "Figure 1 Proportion of missing values in Airbnb data"
  ) +
  coord_flip()
```

In the above visualization, We can identify that the columns `neighbourhood_group_cleansed`, `calendar_updated` and `bathrooms` are completely empty and can be discarded safely. Similarly, `neighbourhood`, `overview` and `host_about` has large portions of missing values as well.

I as interesting in finding the top Airbnb hosts in Chicago that had the most listings. For achieving this tasks, I generated the data for top hosts using `dplyr` library and generated the bar chart using `ggplot2` library as below:

```{r visual-top-hosts, fig.align="center", echo = TRUE, layout="l-body-outset", fig.width = 12, fig.height = 10}
# Generate data for top hosts
top_hosts <- listings %>%
  select(host_id, host_name) %>%
  mutate(name = paste0(host_name, "\n", "(", host_id, ")")) %>%
  count(name, sort = TRUE, name = "count") %>%
  slice(1:10)

# Render bar chart
top_hosts %>%
  # Feed the data to ggplot function
  ggplot(
    aes(
      x = reorder(as.factor(name), -count),
      y = count,
      fill = reorder(name, count)
    )
  ) +
  # Add bar line with no legend
  geom_bar(stat = "identity", width = 0.6, alpha = .8, show.legend = FALSE) +
  # Add label to each bar to show count
  geom_label(
    mapping = aes(label = count), size = 10, fill = "#FFFFFF", fontface = "bold"
  ) +
  # Fill the color with highest value with red to draw attention
  scale_fill_manual(values = rep(c("#2C3E50", "#E74C3C"), times = c(9, 1))) +
  # Apply custom theme
  theme(
    axis.text.x = element_text(size = 9, face = "bold"),
    axis.text.y = element_text(size = 8, face = "bold"),
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10),
    plot.title = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 0.5),
  ) +
  labs(
    # Add x axis label
    x = "Hosts",
    # Add y axis label
    y = "Listings",
    # Add title
    title = "Top 10 Airbnb hosts in Chicago",
    # Add subtitle
    caption = "Figure 2 Top 10 Airbnb hosts in Chicago"
  ) +
  # Flip the co-ordinates
  coord_flip()
```

The above bar chart shows the top hosts in Chicago city. We can observe that host with name `Rob` and id `3965428` has the most number of listings of property. I have also used `ggplot2::scale_fill_manual` function to map red color to highest host with most listing that will allow the viewers to quickly draw attention. Similarly, `ggplot2::theme` function is used to generate a custom theme.

Similarly, for the next tasks, I created a box plot visualization using `ggplot2::geom_boxplot` function to analyze the price of top expensive neighbourhoods.

```{r fig.align="center", echo = TRUE, fig.height = 8}
# Generate data for expensive neighbourhoods
expensive_neighbourhoods <- listings %>%
    select(neighbourhood, price) %>%
    group_by(neighbourhood) %>%
    summarise(average_price = mean(price)) %>%
    arrange(desc(average_price)) %>%
    slice(1:10)

# Compare neighbourhood and prices
neighbourhood_price <- listings %>%
  select(neighbourhood, price) %>%
  filter(neighbourhood %in% expensive_neighbourhoods$neighbourhood) %>%
  mutate(price = log1p(as.numeric(price)))

neighbourhood_price %>%
  ggplot(aes(x = neighbourhood, y = price, fill = neighbourhood)) +
  geom_boxplot(show.legend = FALSE) +
  # Apply custom theme
  theme(
    axis.text.x = element_text(size = 8, face = "bold"),
    axis.text.y = element_text(size = 8, face = "bold"),
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10),
    plot.title = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 0.5),
  ) +
  labs(
    # Add x axis label
    x = "Neighbourhoods",
    # Add y axis label
    y = "Price",
    # Add title
    title = "Distribution of price for top 10 expensive neighbourhoods",
    # Add subtitle
    caption = "Figure 3 Distribution of price for top 10 expensive neighbourhoods"
  ) +
  # Flip the coordinates
  coord_flip()
```

From the above box plot graph, we have good sense of how the data is spread across. We can also observe outliers in prices for some neighbourhoods like Near North side which can skew the distribution of the data. We can also observe that the loop neighbourhood has the highest median price.

For the World Happiness dataset, I was curious in finding the correlation between various variables. First, I imported the data using `reader::read_csv` function and managed the data using `dplyr` library.

```{r data-happiness-report-2021}
happiness_2021 <- readr::read_csv(
  here::here("data", "world_happiness_report_2021.csv")
)
happiness_all <- readr::read_csv(
  here::here("data", "world_happiness_report.csv")
)

happiness_2021 <- happiness_2021 %>%
  rename_with(.fn = ~ tolower(.x)) %>%
  select(!starts_with("Explained by")) %>%
  select(!"dystopia + residual") %>%
  rename_with(.fn = ~ gsub(" ", "_", .x))

happiness_all <- happiness_all %>%
  rename_with(.fn = ~ tolower(gsub(" ", "_", .x)))
```

Next, I generated the correlation matrix using `GGally::ggcorr` function as below:

```{r visual-happiness-report-correlation, layout="l-body-outset", fig.height = 3}
happiness_2021 %>%
  # Rename the columns to fit in the visualization
  select(
    Corruption = perceptions_of_corruption,
    Generosity = generosity,
    Freedom = freedom_to_make_life_choices,
    Life.Expectancy = healthy_life_expectancy,
    Social.Support = social_support,
    GDP = logged_gdp_per_capita,
    Happiness = ladder_score
  ) %>%
  # Render the correlation graph
  ggcorr(
    method = c("everything", "pearson"),
    size = 3,
    hjust = 0.77,
    low = "#CD5C5C",
    mid = "white",
    high = "#9FE2BF",
    label = TRUE,
    label_size = 3,
    layout.exp = 1
  ) +
  # Apply custom theme
  theme(
    plot.title = element_text(size = 10, hjust = 0.5),
    legend.text = element_text(size = 8),
    plot.caption = element_text(size = 10, hjust = 0.5),
  ) +
  labs(
    title = "Correlation Matrx",
    caption = "Figure 4 Identify correlation of Happiness with different variables"
  )
```

From the above correlation matrix graph, we can identify that happiness mostly correlates with GDP, Social Support, Life Expectancy and Freedom.

Similarly, I was also interested in finding the regions that had the most happiest countries. To achieve this, I used `waffle::geom_waffle()` to generate a square chart visualizations that helped me to identify the parts of a whole for world happiness data.

The `dplyr` library is used to generate the data as below:

```{r visual-happiness-report-comparision-data, code_folding=FALSE}
# dimensions
dimensions <- c(
  "ladder_score",
  "logged_gdp_per_capita",
  "social_support",
  "healthy_life_expectancy",
  "freedom_to_make_life_choices",
  "generosity",
  "perceptions_of_corruption"
)

happiness_2021_tranformed <- happiness_2021 %>%
  select(country = country_name, all_of(dimensions)) %>%
  mutate(absence_of_corruption = 1 - perceptions_of_corruption) %>%
  pivot_longer(
    cols = c(all_of(dimensions), "absence_of_corruption"),
    names_to = "dimension",
    values_to = "score"
  ) %>%
  filter(dimension != "perceptions_of_corruption") %>%
  group_by(dimension) %>%
  mutate(
    min_value = min(score),
    max_value = max(score)
  ) %>%
  mutate(score_pct = (score - min_value) / (max_value - min_value)) %>%
  ungroup()


# map country to regions
country_region <- happiness_2021 %>%
  select(country = country_name, region = regional_indicator) %>%
  unique()

happiness_waffle <- happiness_2021_tranformed %>%
    filter(dimension == 'ladder_score') %>%
    left_join(country_region, by = 'country') %>%
    mutate(score_bin = cut(score, seq(2,8, 1), right = FALSE)) %>%
    group_by(region) %>%
    mutate(region_avg = mean(score)) %>%
    ungroup() %>%
    mutate(region = reorder(region, region_avg)) %>%
    count(region, score_bin, name = "count") %>%
    arrange(score_bin, count)

score_levels = levels(happiness_waffle$score_bin)
```

The square chart (waffle chart) is created using `geom_waffle` as below:

```{r visual-happiness-report-comparision, layout="l-body-outset", fig.height = 5}

pal <- colorRampPalette(
  c(
    "#BF360C", # Dark muted color to represent saddness 
    "white", # Middle point
    "#FFBF00" # Yellow color represents happiness
  )
)

ggplot(happiness_waffle, aes(fill = score_bin, values = count)) +
  geom_waffle(color = "white", size = .3, n_rows = 6, flip = TRUE) +
  facet_wrap(
    ~region, nrow = 2, strip.position = "bottom", labeller = label_wrap_gen()
  ) +
  scale_x_discrete() +
  scale_y_continuous(
    labels = function(x) x * 6, # make this multiplyer the same as n_rows
    expand = c(0, 0.2),
    limits = c(0, 7)
  ) +
  scale_fill_manual(
    name = "Yellow suggests more happiness",
    values = pal(6),
    labels = c("", "", "", "", "", "")
  ) +
  labs(
    y = NULL,
    title = "Happiness by World Region",
    caption = "Figure 5 Comparision of happiness by world regions",
  ) +
  theme(
    plot.title = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(size = 10, hjust = 0.5),
    axis.text.y = element_blank(),
    strip.text.x = element_text(size = 6, hjust = 0.5),
    legend.position = "bottom",
    legend.key.size = unit(0.5, "cm"),
    legend.title = element_text(size = 8, hjust = 0.5),
  ) +
  guides(fill = guide_legend(reverse = FALSE, nrow = 1, byrow = TRUE))
```

From the above visualization, we can quickly identify that North America and Western Europe is the happiest region with most countries followed by Central and Eastern Europe. The color yellow is used to represent the happiness where as dark color is used to denote the opposite.

Lastly, I wanted to compare the happiness between the pre-covid year and amongst-covid year to see If there was any changes in happiness index. For this tasks, I created a dumbbell chart using `ggplot2::geom_segment` functions.

The code to generate the data is below:

```{r visual-happiness-report-comparision-year-data, code_folding=FALSE}

happiness_covid_2019_2020 <- happiness_all %>%
  filter(year >= 2019) %>%
  left_join(country_region, by = c("country_name" = "country")) %>%
  select(country = country_name, region, year, ladder = life_ladder) %>%
  pivot_wider(
    names_from = "year",
    names_prefix = "year",
    values_from = "ladder"
  ) %>%
  filter(!is.na(year2019) & !is.na(year2020)) %>%
  group_by(region) %>%
  summarize(
    happiness_2019 = mean(year2019, na.rm = TRUE),
    happiness_2020 = mean(year2020, na.rm = TRUE)
  ) %>%
  mutate(diff = happiness_2020 - happiness_2019) %>%
  arrange(diff) %>%
  mutate(region = factor(region, levels = region))

happiness_covid_2019_2020 <- happiness_covid_2019_2020 %>%
  pivot_longer(cols = c(happiness_2019, happiness_2020)) %>%
  rename(happiness = value) %>%
  mutate(year = as.factor(parse_integer(str_extract(name, "\\d*\\.?\\d+")))) %>%
  mutate(x_pos = happiness + (diff/2)) %>%
  select(!name)

happiness_covid_2019 <- happiness_covid_2019_2020 %>% filter(year == 2019)
happiness_covid_2020 <- happiness_covid_2019_2020 %>% filter(year == 2020)

```

The code to generate the dumbbell chart using `ggplot2::geom_segment` is below:

```{r visual-happiness-report-comparision-year-graph, layout="l-body-outset", fig.width=8}

ggplot(happiness_covid_2019_2020) +
  geom_segment(
    data = happiness_covid_2019,
    x = happiness_covid_2019$happiness,
    y = happiness_covid_2019$region,
    xend = happiness_covid_2020$happiness,
    yend = happiness_covid_2020$region,
    color = "#aeb6bf",
    size = 6,
    # Note that I sized the segment to fit the points
    alpha = .5
  ) +
  geom_point(
    data = happiness_covid_2019_2020,
    aes(x = happiness, y = region, color = year),
    size = 6,
    show.legend = TRUE
  ) +
  geom_text(
    data = happiness_covid_2019,
    aes(
      label = paste("D: ", round(diff, 2)),
      x = x_pos,
      y = region
    ),
    color = "#4a4e4d",
    size = 2
  ) +
  geom_text(
    data = happiness_covid_2019 %>% filter(diff > 0),
    color = "black",
    size = 2,
    hjust = 2,
    aes(
      x = happiness,
      y = region,
      label = round(happiness, 2)
    )
  ) +
  geom_text(
    data = happiness_covid_2020 %>% filter(diff > 0),
    color = "black",
    size = 2,
    hjust = -1,
    aes(
      x = happiness,
      y = region,
      label = round(happiness, 2)
    )
  ) +
  geom_text(
    data = happiness_covid_2019 %>% filter(diff < 0),
    color = "black",
    size = 2,
    hjust = -1,
    aes(
      x = happiness,
      y = region,
      label = round(happiness, 2)
    )
  ) +
  geom_text(
    data = happiness_covid_2020 %>% filter(diff < 0),
    color = "black",
    size = 2,
    hjust = 2,
    aes(
      x = happiness,
      y = region,
      label = round(happiness, 2)
    )
  ) +
  # color points
  scale_color_manual(values = c("#FF7F50", "#9FE2BF")) +
  labs(
    x = NULL,
    y = NULL,
    title = "Comparision of Happiness: pre-COVID(2019) to amongst-COVID(2020)"
  ) +
  theme(
    axis.text.x = element_text(size = 5, face = "bold"),
    axis.text.y = element_text(size = 6, face = "bold"),
    plot.title = element_text(size = 10, hjust = 0.5),
  )
```

From the above dumbbell graph, we can see that there was an increase in some of the world regions like South Asia, Sub-Saharan Africa in-spite of COVID pandemic.

In addition, I have generated additional summaries for the dataset to gain insight on various columns and its quality.

```{r numerical-summaries-airbnb-listings}

listings_summary <- listings %>%
  group_by(neighbourhood) %>%
  summarise(
    mean_price = mean(price),
    median_price = median(price),
    minimum_price = min(price),
    maximum_price = max(price),
    sd_price = sd(price)
  )

listings_summary %>% 
  paged_table(options = list(rows.print = 15))
```

Lastly, I can implement a regression model for descriptive analysis. I created numerical summaries for the World happiness data for the quantitative variables `ladder_score` and `logged_gdp_per_capita`.

```{r visual-regression-summary-1}
summary(happiness_2021$ladder_score)
```

```{r visual-regression-summary-2}
summary(happiness_2021$logged_gdp_per_capita)
```

Lets check if the ladder_score variable follows a normal distribution using `hist` function.

```{r visual-regression-hist}
hist(happiness_2021$ladder_score)
```

The above observation is roughly bell-shaped as most observation is in the middle of distribution.

Also, I tested the relationship between the variable ladder_score and logged_gdp_per_capita to see if the relationship is linear.

```{r visual-regression-plot-relationship}
plot(ladder_score ~ logged_gdp_per_capita, data = happiness_2021)
```

The above relationship looks roughly linear.

Next, I performed a linear regression analysis using linear model `lm` function.

```{r visual-regression-show-summary}
gdp_ladder_lm <- lm(ladder_score ~ logged_gdp_per_capita, data = happiness_2021)

summary(gdp_ladder_lm)
```

In the above output, we can identify the min, max, first quartile, median, third quartile and max values along with coefficients.

```{r visual-regression-plot-linear-model}
par(mfrow=c(2,2))
plot(gdp_ladder_lm)
par(mfrow=c(1,1))
```

The red lines representing the mean of residuals are mostly horizontal which idicates minimal outliers or biases. The Normal Q-Q graph demonstrates the real residuals from the model we created in above section showing 1:1 line.

### 2.2.1 Conclusion

I believe I have met this objective as I have created different visualizations using the `ggplot2`, `waffle`, and `GGally` libraries. By creating the visualizations for various tasks, I am able to explore important characteristics of a data and draw conclusions from them. Likewise, I have combined multiple visualization created above into my shiny app project to create effective data project. I have also created my own theme using `ggplot2::theme` function to highlight key features of data. I have completed exploratory analysis of the data by generating the correlation data among variables and generate heat map matrix to draw conclusions. Further, I have generated numerical summaries such as mean price for each neighbourhoods using `dplyr::summarise` and `dplyr::group_by` and visualize it using box plot for additional descriptive analysis.

```{r visual-clean-up, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
cleanup_variables = c(
  "expensive_neighbourhoods",
  "listings",
  "listings_raw",
  "listings_summary",
  "reviews",
  "reviews_raw",
  "neighbourhood_price",
  "missing_stat",
  "missing_values_summary",
  "top_hosts",
  "happiness_2021",
  "happiness_all",
  "happiness_2021_tranformed",
  "happiness_covid_2019_2020",
  "happiness_covid_2019",
  "happiness_covid_2020",
  "happiness_waffle",
  "country_region"
)
rm(list = cleanup_variables)
```

------------------------------------------------------------------------

## 2.3 Write R programs for simulations from probability models and randomization-based experiments

### 2.3.1 Simulations

#### Data Overview

```{r simulations-storms-dataset}
storms %>% 
  paged_table(options = list(rows.print = 15))
```

For the above dataset, we can sample random rows by generating the indices of the rows into the object.

```{r simulatons-sample-data}
# Create indices
ind <- seq_len(nrow(storms))

# Obtain a sample of 15
sample_ind <- sample(ind, 15)

# Display the sample results
storms[sample_ind, ] %>%
  paged_table(options = list(rows.print = 15))
```

For the storms dataset, lets have a look at the mean and standard deviation.

```{r simulations-data-stats}
mean(storms$pressure)
sd(storms$pressure)
```

#### Storm pressure simulation

In the code snippet below, I have performed a simulation for the air pressure at storm center by generating 100 random samples of a normally distributed variables using `rnorm` with a mean of 990 and a standard deviation of 19. Also, I have generated multiple samples of data using `map` function.

```{r simulations-storm-pressure-simulation}
# https://github.com/gvsu-sta518/activity11-simulation
# Set seed for consistent results
set.seed(415)

generate_sample_mean <- function(data) {
  data %>%
    summarise(across(everything(), mean, .names = "mean_{.col}"))

  return(data)
}

generate_samples <- function(n = 100) {
  samples <- tibble(
    normal = rnorm(n, mean = 990, sd = 19),
    exponential = rexp(n = n, rate = 0.2),
    binomial = rbinom(n = n, size = 100, prob = 0.25),
    uniform = runif(n = n, min = 10, max = 20)
  )
  return(samples)
}

pressure_samp_1000 <- generate_samples()
```

Lets view some of the descriptive statistics for sampled air pressure values:

```{r simulations-storm-pressure-values}
summary(pressure_samp_1000$normal)

summary(pressure_samp_1000$exponential)
```

```{r}
samples <- map(1:1000, generate_samples)

samples_means <- map_df(samples, generate_sample_mean)

restructured_means <- samples_means %>% 
    pivot_longer(
      cols = everything(),
      names_to = "distribution",
      values_to = "means"
    )

restructured_means %>% 
    ggplot(aes(x = means)) +
    geom_histogram(aes(y = ..count.. / sum(..count..))) +
    facet_wrap(~ distribution, ncol = 2, nrow = 2, scales = "free") +
    labs(y = "proportion")
```

```{r}
restructured_means %>%
  group_by(distribution) %>%
  summarise(
    mean = mean(means),
    sd = sd(means)
  ) %>%
  knitr::kable()
```

```{r simulations-storm-pressure-boxplot}
ggplot(pressure_samp_1000, aes(y=normal)) + 
  geom_boxplot()
```

```{r simulation-clean-up, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
clean_up_variables <- c(
  "pressure_samp_1000",
  "samples",
  "samples_means"
)
rm(list = clean_up_variables)
```

### 2.3.2 Bootstrapping

#### Introduction

Bootstrapping can be useful to draw a valid inference when the sample size is small. We can use a re-sampling method with replacement to create any number of re-samples. For implementing bootstrapping, I came across a useful package [boot](https://www.rdocumentation.org/packages/boot/versions/1.3-28/topics/boot) that generates bootstrap replicates of a statistics applied to data.

#### Load Packages

```{r bootstrapping-load-package}
library(boot)
```

#### Data Overview

To implement bootstrap re-sampling, I've used built-in `Loblolly` dataset that contains the data about the growth of the Loblolly pine tree.

```{r bootstrapping-data-overview}
data("Loblolly", package = "datasets")
knitr::kable(head(Loblolly))
```

```{r bootstrapping-data-summary}
summary(Loblolly)
```

```{r bootstrapping-statistics-function}
# A function which when applied to data returns a vector 
# containing the statistic(s) of interest.
statistics_function <- function(data, i){
 d2 <- data[i,] 
 return(cor(d2$height, d2$age))
}
```

In the above code, I've defined a custom function `statistics_function` that accepts two parameters, namely, `data` and `i`. The `data` parameter denotes the dataset as the name suggest where as `i` parameter is the index denoting the row of the data that will be used to create bootstrap sample.

```{r bootstrapping-correlation}
set.seed(518)
bootstrap_correlation <- boot(Loblolly, statistic = statistics_function, R = 10000)
```

Now, we apply the `boot` function which accepts dataset, statistic function and number of bootstrap sample. The `seed` function is used in this context to generate reproducible results.

```{r bootstrapping-correlation-output}
bootstrap_correlation
```

We can observe that the correlation between two variable is 0.989 with a standard error of 0.0016.

```{r bootstrapping-correlation-overview}
summary(bootstrap_correlation)
```

Similarly, we can view the value of the `boot` object.

Below, we have generated range, mean and standard deviation of bootstrap samples.

```{r bootstrapping-additional-statistics-range}
range(bootstrap_correlation$t)
```

The bootstrap correlation ranges from 0.982 to 0.995

```{r bootstrapping-additional-statistics-mean}
mean(bootstrap_correlation$t)
```

The mean for bootstrap correlation is 0.989.

```{r bootstrapping-additional-statistics-sd}
sd(bootstrap_correlation$t)
```

The standard deviation for bootstrap correlation is 0.001.

```{r bootstrapping-confidence-interval}
boot.ci(boot.out=bootstrap_correlation,type=c('norm','basic','perc','bca'))
```

The above function call `boot.ci` demonstrates the confidence interval of 4 types: normal, basic, percentile and bca distribution.

### 2.3.2 Conclusions

I believe I have fulfilled the objective as I've performed simulation, bootstrapping as well as completed all the course activities. As demonstrated in the above code, I can write R scripts to perform simulations in various tasks where we need to introduce some randomness by generating random numbers using sampling techniques. For performing simulation in R, I can use `rnorm`, `dnorm`, `rexp` and `rbinom`. I have also performed bootstrapping to implement re-sampling methods and draw conclusions on data.

```{r bootstrapping-clean-up, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
cleanup_variables = c(
  "Loblolly",
  "bootstrap_correlation"
)
rm(list = cleanup_variables)
```

------------------------------------------------------------------------

## 2.4 Use source documentation and other resources to troubleshoot and extend R programs

### 2.4.1 Troubleshoot R programs

Coming from mindset of Python programming and having a limited knowledge of R, I faced a lot of challenges and issues when developing my projects and exploring R. Moreover, I also found identifying error message to be a time-consuming activity. I did a lot of search using Google, explored stackoverflow.com and used source documentation of libraries/packages. After a while, I got much better at detecting and fixing common errors by writing and practicing more R code.

I have also been reading the source documentation and learning resources such as [R for Data Science](https://r4ds.had.co.nz/index.html), [R Manuals](https://cran.r-project.org/manuals.html) and YouTube. The Help tab on RStudio application been super useful in quickly reading the documentation and find usage information for specific functions or library.

For me, the most common error was Syntax errors. I would often miss to load the library, forget about closing a bracket or misspell the words. One best practice to quickly identify the error was to break the code into multiple lines.

For example, consider below code:

    Orange %>% select(Tree, age, circumference) %>% group_by(Tree) %>% summarize(avg_circumference=mean(circumference))

The above code is much less readable as everything is in single-line. Breaking above code into multiple lines can significantly improve readability and at the same time identifying issues can be easier.

    Orange %>%
      select(Tree, age, circumference) %>%
      group_by(Tree) %>%
      summarize(avg_circumference=mean(circumference))

Another quick and reliable way to identify and resolve errors is to use `tryCatch` statement. Below is the snippet of code I've used in my [scrape](https://github.com/sajalshres/sta-518-project/blob/main/modules/scrape.R) function script to quickly identify if something goes wrong.

``` r
tryCatch(
  download.file(link, dest_file),
  # handle warning
  warning = function(warn) {
    print(paste("Warnign: ", warn))
  }
  # handle error
  error = function(error) {
    print(paste0("Error occurred when downloading file. reason: ", error))
    is_downloaded <<- FALSE
  }
)
```

I also found that collaborating with my team and friends is another great way to troubleshoot issues. I collaborated on numerous occasion with my team on troubleshooting on various issues and helping them when needed. I also ask for help on specific issues when I am stuck.

### 2.4.2 Extend R programs

#### Functions

Functions is an excellent way of generating a reproducible piece of code that can be used numerous times in your code. I have built a user defined function that is used to search the github repositories. The function `search_github_repositories` accepts three parameters:

-   **query**: The search keywords.
-   **sort**: Sorts the results of your query by number of stars, forks, or help-wanted-issues
-   **order**: Determines whether the first search result returned is the highest number of matches (desc) or lowest number of matches (asc).

The function makes an http request using the `httr` library to the github api endpoint. Next, the status code is checked and upon success, the response is parsed.

```{r extend-r-user-function-definition}
search_github_repositories <-
  function(query, sort = "stars", order = "desc") {
    # urls
    github_api_url <- "https://api.github.com"
    search_endpoint <- "/search/repositories"
    
    # build query params
    query_params <-
      paste(paste0("?q=", query),
            paste0("sort=", sort),
            paste0("order=", order),
            sep = "&")
    
    print(paste0("Url is ",github_api_url, search_endpoint, query_params))
    
    # make http request to Github Api
    response <-
      httr::GET(url = paste0(github_api_url, search_endpoint, query_params))
    
    response_status_code <- httr::status_code(response)
    
    # Stop if unexpected status code
    if (response_status_code != 200) {
      stop(paste0("Unexpected status", response_status_code, "encountered"))
    }
    
    # Parse the JSON response
    response_parsed <- httr::content(response, as = "parsed")
    
    # Print stats
    print(paste(
      "Search for",
      query,
      "found",
      response_parsed$total_count,
      "results"
    ))
    
    return(response_parsed)
  }
```

The below code chuck demonstrate how to call the created function.

```{r user-function-call}
# Search for repositories with name sta-518
search_response <- search_github_repositories(query = "sta-518")
search_response$items[[1]]$full_name
search_response$items[[1]]$owner$url
```

```{r extend-r-function-clean-up, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
cleanup_variables = c(
  "search_response"
)
rm(list = cleanup_variables)
```

#### Packages

Packages are the awesome way of sharing and distributing R code that help us to extend the capabilities of R programs and collaborate with others. Throughout my journey to learn R, I've extensively used packages like `dplyr`, `tibble`, `ggplot2`, `boot` etc. I cannot imagine developing R code without support for packages as it made my life much easier and saved a lot of time. So, I wanted to create a custom package to learn how to create packages and install them.

I created a package SearchGithubRepository using the code from functions artifacts.

The Github link to package is: <https://github.com/sajalshres/SearchGithubRespositories>

##### Load Libraries

I've installed and used `devtools` library to install custom package as it is not published yet.

```{r extend-r-load-package}
library(devtools)
```

##### Load my custom package

I've used `install_github` function to install the `SearchGithubRespositories` package.

```{r extend-r-load-custom-package}
install_github("sajalshres/SearchGithubRespositories")
```

##### Sample usage

Below is the demonstration of the usage of the package:

```{r extend-r-custom-package-usage}
SearchGithubRespositories::version()
```

```{r extend-r-custom-package-usage-2}
response <- SearchGithubRespositories::search_github_repositories(query = "sta-518")
response$total_count
```

Similarly, we can view the help documentation using `?` operator.

```{r extend-r-custom-package-usage-3}
?SearchGithubRespositories::search_github_repositories
```

![](images/help_SearchGithubRespositories.png){width="50%"}

### 2.4.3 Conclusions

I conclude that I've met all the requirement for the objective as I'm able to identify common errors or issues with R programs and troubleshoot them. I also have explored on numerous ways to troubleshoot which included reading package documentations, online search, code formatting, common mistakes, collaborating and using `tryCatch` statements. In addition, I've created numerous functions like `search_github_repositories` and `download_files` in my project. I also created custom package `SearchGithubRespositories` as a cool way to share and extend R programs. Lastly, I have explored and make use of various packages like `GGally`, `wally`, `plumber`, `httpr` etc to complete specific tasks in my analyses.

```{r extend-r-package-clean-up, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
cleanup_variables = c(
  "response"
)
rm(list = cleanup_variables)
```

------------------------------------------------------------------------

## 2.5 Write clear, efficient, and well-documented R programs.

I have dedicated myself to spend good amount of time writing R code that included scripts, functions, packages, `rmarkdown` files and so on. I have developed my portfolio website using `distill` library that uses `rmarkdown` to generate html files. By making use of `rmarkdown` files, I've learned to include markdown contents, code chucks with inline code, tables, figures, output formats etc.

### 2.5.2 Naming Conventions

I've learned about various naming conventions that can be used with R such as camelCase, snake_case, PascalCase etc. I've found that R doesn't have a strict naming conventions like Java, Python. But it is best to use conventions that yields meaningful and readable names. The `snake_case` works best for me as I find it more readable and easier for my issues. For function names, I prefer to use lower case with periods like test.case1.

### 2.5.3 Project Structure

I have also learned to structure a R project. A good content organizations can help us to quickly navigate in a complex enterprise projects and can be scaled very easily. For my final project, I've used below structure to simulate a complex application:

    sta-518-project
    |
    |-- api           # API application to fetch raw data, analaysis and summaries.
    |-- app           # Shiny web appliation containing interactive dashboard.
    |-- data          # Contains raw and processed data
    |   |-- raw
    |   |-- processed
    |-- docs          # Contains documentation and rmarkdown notebooks.
    |-- etl           # ETL cli application to scrape and process the data for analysis.
    |-- modules       # Shared modules used by scripts and applications.
    |-- scripts       # Contains rscripts.

### 2.5.4 Functions and Packages

Functions and packages helps to breakdown complex piece of code into a single tasks that can be reused multiple times with out duplicating them. When developing ShinyApp for my project, I've created functions and scripts that can be used to complete various tasks for analysis. To accomplish this, I've used `source` function that help to reuse function that is created in another R script.

For instance, in my shiny application, I've break-up my code into small independent code and used them as:

    # Read modules
    source("modules/maps.R")
    source("modules/analyze.R")

### 2.5.5 Documenting Code

I believe the most important aspect of documenting your code is to introduce comments. Comments helps to summarize the piece of code, provides notes to any important decision made or explain why something is implemented the way it is. It acts as a note for self in the future and make collaborating with other much easier.

In addition, it is best to name the chuck of code when using R Markdown. The rstudio can make navigation much easier and we can generate table of content.

### 2.5.6 Conclusions

I believe I've achieved this objective as I am able to create complex project based workflows and create professional reports using `rmakrdown`. I also have divided my project code self-contained functions to perform analysis. Similarly, I've documented my code using comments to provide summaries, resources and explanation for taking specific approach.
